{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## http://static.wooridle.net/lectures/chatbot/\n",
    "#https://teamlab.github.io/jekyllDecent/blog/tutorials/%EC%B9%B4%EC%B9%B4%EC%98%A4%ED%86%A1_%EC%B1%97%EB%B4%87_%EB%A7%8C%EB%93%A4%EA%B8%B0_with_python_flask_aws\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "naver_reviews_list = pd.read_excel('naver_reviews_list.xlsx')\n",
    "blog_list = pd.read_excel('blog_list_df.xlsx')\n",
    "google_review = pd.read_excel('google_review.xlsx')\n",
    "insta_review = pd.read_excel('insta_info_df.xlsx')\n",
    "\n",
    "blog_list_1 = blog_list[['nm', 'blog_title']]\n",
    "blog_list_1.columns = ['nm', 'review_contents']\n",
    "\n",
    "blog_list_2 = blog_list[['nm', 'blog_contents']]\n",
    "blog_list_2.columns = ['nm', 'review_contents']\n",
    "\n",
    "blog_list_re = pd.concat([blog_list_1, blog_list_2], ignore_index=True)\n",
    "\n",
    "google_list_re = google_review[['nm', 'google_contents']]\n",
    "google_list_re.columns = ['nm', 'review_contents']\n",
    "\n",
    "insta_review_re = insta_review[['location_info', 'main_text']]\n",
    "insta_review_re.columns = ['nm', 'review_contents']\n",
    "\n",
    "train_df = pd.concat([blog_list_re, insta_review_re, naver_reviews_list[['nm', 'review_contents']], google_list_re], ignore_index=True)\n",
    "\n",
    "train_df['review_contents'] = train_df['review_contents'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_df['review_contents'] = train_df['review_contents'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_df['review_contents'].replace('', np.nan, inplace=True)\n",
    "train_df = train_df.dropna(axis=0)\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "#객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "train_df.isnull().any() #document에 null값이 있다.\n",
    "train_df['review_contents'] = train_df['review_contents'].fillna(''); #null값을 ''값으로 대체\n",
    "\n",
    "def tokenize(doc):\n",
    "    #형태소와 품사를 join\n",
    "    \n",
    "    ##불용어\n",
    "    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를', '을', '으로','자','에','와','한','하다', '대', '전', '님', '에서', '때', '로', '고']\n",
    "    \n",
    "    ##품사(원하는 품사만 가져오기)\n",
    "    tagset = ['Adjective', 'Noun', 'Exclamation'] \n",
    "    #okt.pos(doc, norm=True, stem=True)\n",
    "    \n",
    "    ##토근\n",
    "    temp_X = okt.pos(doc, norm=True, stem=True)\n",
    "    #return ['/'.join(word) for word in temp_X if not word[0] in stopwords if word[1] in tagset] # 불용어 제거\n",
    "    return [word[0] for word in temp_X if not word[0] in stopwords if word[1] in tagset] # 불용어 제거 #원하는 품사만 \n",
    "\n",
    "#tokenize 과정은 시간이 오래 걸릴수 있음...\n",
    "train_docs = [(tokenize(row[1])) for row in  train_df.values]\n",
    "\n",
    "import nltk\n",
    "#tokens = [t for d in train_docs for t in d[0]]\n",
    "tokens = [t for d in train_docs for t in d]\n",
    "\n",
    "text = nltk.Text(tokens, name='NMSC')\n",
    "\n",
    "#토큰개수\n",
    "print(len(text.tokens))\n",
    "\n",
    "#중복을 제외한 토큰개수\n",
    "print(len(set(text.tokens)))\n",
    "\n",
    "#출력빈도가 높은 상위 토큰 10개\n",
    "print(text.vocab().most_common(50))\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "text.plot(50)\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=train_docs, vector_size =100, window=5, min_count=5, workers=4, sg=0)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('word2vec') # 모델 저장\n",
    "\n",
    "# annotation text 만들기 (시각화할 때 벡터 말고 단어도 필요하니까)\n",
    "# vocabs = word_vectors.vocab.keys()\n",
    "text=[]\n",
    "word_vectors = model.wv\n",
    "vocabs = model.wv.index_to_key \n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "xys = pca.fit_transform(word_vectors_list)\n",
    "xs = xys[:,0]\n",
    "ys=xys[:,1]\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fm._rebuild()\n",
    "\n",
    "plt.rc('font', family='NanumGothic')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_graph(vocabs, xs, ys):\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(xs,ys,marker='o')\n",
    "    for i,v in enumerate(vocabs):\n",
    "        plt.annotate(v,xy=(xs[i], ys[i]))\n",
    "        \n",
    "plot_2d_graph(vocabs, xs,ys)\n",
    "\n",
    "for i,v in enumerate(vocabs):\n",
    "    text.append(v)\n",
    "    \n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Scatter(x=xs,\n",
    "                                y=ys,\n",
    "                                mode='markers+text',\n",
    "                                text=text)) \n",
    "\n",
    "fig.update_layout(title='Naver Word2Vec')\n",
    "fig.show()\n",
    "\n",
    "plotly.offline.plot(\n",
    "fig, filename='naver_word2vec.html'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "##### 임베딩을 위한 데이터 저장하기\n",
    "##https://soohee410.github.io/embedding_projector\n",
    "##https://projector.tensorflow.org/\n",
    "##해당사이트에서 tensor와 meta파일을 읽어오기\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors  \n",
    "model.wv.save_word2vec_format('naver_w2v')\n",
    "\n",
    "!python -m gensim.scripts.word2vec2tensor --input naver_w2v --output naver_w2v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 모든병원별 형태소 text list 만들기\n",
    "hp_all = pd.read_excel('results.xlsx')\n",
    "\n",
    "for hp_nm in hp_all['name']:\n",
    "    globals()[hp_nm] = nltk.Text([t for d in [(tokenize(row[1])) for row in  train_df[train_df['nm'] == hp_nm].values] for t in d], name='NMSC').vocab().most_common(1000)\n",
    "    #print(hp_nm)\n",
    "    \n",
    "def hp_rank_output(message):\n",
    "    try:\n",
    "        ##https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
    "        # 유사도가 높은 단어 추출\n",
    "\n",
    "        ##검색어 입력받기(끊어서 명사 혹은 동사 등으로 검색해야 잘검색됨)\n",
    "        #input_value = input()\n",
    "        input_value = message\n",
    "\n",
    "        ##tokenize 함수로 명사와 동사 그리고 감탄사만 가져와서 검색하기\n",
    "        search_text = tokenize(input_value)\n",
    "        #print(search_text)\n",
    "\n",
    "        # 유사도가 높은 단어 조회\n",
    "        key_table = pd.DataFrame(model.wv.most_similar(search_text, topn=10), columns=['key', 'value'])\n",
    "        key_table = key_table.set_index(key_table['key'])\n",
    "\n",
    "        ### 직접 언급한 키워드의 유사도는 3로 고정함(기본 유사도가 99%이기 때문에 이보다 3배 더 고려하기 위함)\n",
    "        for x in search_text:\n",
    "            key_table.loc[x] = [x, 3]\n",
    "\n",
    "        search_list = []\n",
    "\n",
    "        ## 유사어\n",
    "        for x in model.wv.most_similar(search_text, topn=10):\n",
    "            search_list.append(x[0])\n",
    "\n",
    "        ## 검색어\n",
    "        for x in search_text:\n",
    "            search_list.append(x)\n",
    "\n",
    "        #for hp_nm in hp_all['name']:\n",
    "        #    print(hp_nm, [word for word in globals()[hp_nm] if word[0] in search_list] )    \n",
    "\n",
    "        ## 평점계산\n",
    "        socre_table = {}\n",
    "\n",
    "        for hp_nm in hp_all['name']:\n",
    "            token_cnt_list = []\n",
    "            for token in globals()[hp_nm]:\n",
    "                token_cnt_list.append(token[1])\n",
    "\n",
    "                ###검색어가 무조건 다 있는 상태에서 순위를 알려줘야 함\n",
    "                #ex) 친절한 정형외과 -> 친절하다, 정형외과\n",
    "                #    두가지 형태소가 모두 존재하는 병원에서 평점을 계산하가 \n",
    "                #    두가지 중 하나라도 없으면 점수를 대폭 낮추거나 0점 처리하기\n",
    "                #    두가가 중 하나라도 있으면 (해당 동일갯수 비율 * 0.1) ,  모두 없으면 0점처리\n",
    "\n",
    "            #print('검색어 형태소 갯수 : ' + str(len(search_text)))\n",
    "            match_cnt = 0\n",
    "            for x in [word for word in globals()[hp_nm] if word[0] in search_text]:\n",
    "                match_cnt += 1\n",
    "\n",
    "            #print('찾은 형태소 갯수 : ' + str(match_cnt))\n",
    "\n",
    "            match_wgt = match_cnt / len(search_text)\n",
    "            if match_wgt != 1:\n",
    "                match_wgt = match_wgt * 0.1\n",
    "            #print(match_wgt)\n",
    "\n",
    "            value_list = []\n",
    "            for x in [word for word in globals()[hp_nm] if word[0] in search_list]:\n",
    "                value_list.append((key_table.loc[x[0], 'value'].round(2)) * (x[1]))\n",
    "\n",
    "                                                                                ###형태소 비율로 해서 가중치 고려\n",
    "                #value_list.append((key_table.loc[x[0], 'value'].round(2)) * ((x[1]/sum(token_cnt_list)*100)))\n",
    "\n",
    "            try:\n",
    "                socre_table[hp_nm] = ((sum(value_list) * (((naver_reviews_score.loc[hp_nm, 'review_score']+ 5) )/ 10)).round(2)) * match_wgt\n",
    "\n",
    "\n",
    "                                                            ##형태소개수 고려해서 가감\n",
    "                #socre_table[hp_nm] = (sum(value_list)  *  (sum(token_cnt_list)/ 50000)  * (((naver_reviews_score.loc[hp_nm, 'review_score']+ 5) )/ 10)).round(2)\n",
    "            except:\n",
    "                ###review정보에 해당 병원정보가 없어서 평점산출이 안되면 0.5로 고정함..\n",
    "                try:\n",
    "                    socre_table[hp_nm] = ((sum(value_list) * (0.5)).round(2)) * match_wgt\n",
    "                except:\n",
    "                    socre_table[hp_nm] = ((sum(value_list) * (0.5))) * match_wgt\n",
    "\n",
    "\n",
    "            socre_table_sort = sorted(socre_table.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "        #print('검색어: ' +  input_value)\n",
    "        #print('연관검색어: ', search_list)\n",
    "        sort_count = 1\n",
    "        rank_list = \"\"\n",
    "        for x in socre_table_sort:\n",
    "            token_cnt_list = []\n",
    "            for token in globals()[x[0]]:\n",
    "                token_cnt_list.append(token[1])\n",
    "            url_text= hp_all[hp_all['name'] == x[0]]['주소'].values[0]\n",
    "            name_dept= hp_all[hp_all['name'] == x[0]]['name_dept'].values[0]\n",
    "            \n",
    "            if sort_count < 6:\n",
    "                rank_list = rank_list + (str(sort_count) + \"순위:\\n-\" +  x[0] + \"\\n-추천점수: \" + str(x[1].round(2)) + \"\\n-\" +  name_dept + \"\\n-검색어갯수:\" + str(sum(token_cnt_list)) + \"\\n\" +  url_text + \"\\n\") \n",
    "            sort_count += 1\n",
    "    except:\n",
    "        search_list = '관련 검색어가 없습니다'\n",
    "        rank_list = '재검색해주시기 바랍니다.'\n",
    "        \n",
    "    return \"연관검색어\\n '{}'\".format(search_list) + \"\\n\\n검색결과: \\n'{}'\".format(rank_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
